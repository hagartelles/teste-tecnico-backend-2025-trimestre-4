# ğŸ“¦ CEP Crawler - API Documentation

Asynchronous CEP crawling system using queues, MongoDB, and NestJS.

---

## ğŸ—ï¸ Architecture

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â”€â–¶â”‚  API (REST)  â”‚â”€â”€â”€â”€â”€â–¶â”‚  MongoDB    â”‚
â”‚  (Postman)  â”‚      â”‚  NestJS:3000 â”‚      â”‚  :27017     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ ElasticMQ    â”‚
                     â”‚ (SQS) :9324  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    Worker    â”‚
                     â”‚ Rate Limited â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Flow

1. **POST /cep/crawl** â†’ Validates range â†’ Creates MongoDB record â†’ Enqueues CEPs â†’ Returns 202
2. **SQS Queue** â†’ Stores messages (1 per CEP)
3. **Worker** â†’ Consumes queue (350ms between requests) â†’ Queries ViaCEP â†’ Saves result
4. **GET /cep/crawl/:id** â†’ Returns real-time progress
5. **GET /cep/crawl/:id/results** â†’ Returns processed CEPs

**Multiple simultaneous crawls:** Each request generates a unique `crawl_id` and processes independently.

---

## ğŸš€ Initial Setup

### Prerequisites

- Node.js 18+
- Docker & Docker Compose
- Postman (optional)

### 1. Clone and Install

```bash
git clone <repository>
cd teste-tecnico-backend-2025-trimestre-4

npm install
```

### 2. Configure Environment Variables

Configure the file `environments/.dev.env`:

```bash
# Application
NEST_PORT=3000

# MongoDB
MONGO_URI=mongodb://mongo:27017/crawler

# Queue
SQS_ENDPOINT=http://sqs:9324
QUEUE_NAME=cep-queue
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=000000000000
AWS_ACCESS_KEY_ID=x
AWS_SECRET_ACCESS_KEY=x

# ViaCEP
VIACEP_BASE_URL=https://viacep.com.br/ws
VIACEP_TEST_CEP=01001000
```

### 3. Start Containers

```bash
docker-compose up -d --build
```

**Check status:**
```bash
docker-compose ps

# Should show 3 containers UP:
# - mongo (port 27017)
# - sqs (ports 9324, 9325)
# - app (port 3000)
```

### 4. View Logs

```bash
# All services
docker-compose logs -f

# Application only
docker-compose logs -f app
```

---

## ğŸ“¡ API Endpoints

### 1. Create Crawl Request

**`POST /cep/crawl`**

```bash
curl -X POST http://localhost:3000/cep/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "cep_start": "01001000",
    "cep_end": "01001009"
  }'
```

**Response (202 Accepted):**
```json
{
  "crawl_id": "676d5a1b2c3d4e5f6a7b8c9d",
  "message": "Crawl request created successfully",
  "total_ceps": 10
}
```

**Validations:**
- CEPs with 8 numeric digits
- `cep_start <= cep_end`
- Maximum range: 1000 CEPs

---

### 2. Query Status

**`GET /cep/crawl/:crawl_id`**

```bash
curl http://localhost:3000/cep/crawl/676d5a1b2c3d4e5f6a7b8c9d
```

**Response (200 OK):**
```json
{
  "crawl_id": "676d5a1b2c3d4e5f6a7b8c9d",
  "cep_start": "01001000",
  "cep_end": "01001009",
  "total_ceps": 10,
  "processed_count": 10,
  "success_count": 8,
  "error_count": 2,
  "status": "finished",
  "started_at": "2025-12-26T05:52:32.000Z",
  "finished_at": "2025-12-26T05:52:42.000Z",
  "created_at": "2025-12-26T05:52:30.000Z",
  "updated_at": "2025-12-26T05:52:42.000Z"
}
```

**Possible statuses:**
- `pending` - Waiting for processing
- `running` - In progress
- `finished` - Completed
- `failed` - Critical error

---

### 3. Query Results

**`GET /cep/crawl/:crawl_id/results?page=1&limit=10`**

```bash
curl "http://localhost:3000/cep/crawl/676d5a1b2c3d4e5f6a7b8c9d/results?page=1&limit=10"
```

**Response (200 OK):**
```json
{
  "crawl_id": "676d5a1b2c3d4e5f6a7b8c9d",
  "results": [
    {
      "cep": "01001000",
      "success": true,
      "data": {
        "cep": "01001-000",
        "logradouro": "PraÃ§a da SÃ©",
        "bairro": "SÃ©",
        "localidade": "SÃ£o Paulo",
        "uf": "SP",
        "ddd": "11"
      },
      "error_message": null,
      "created_at": "2025-12-26T05:52:33.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 10,
    "total": 10,
    "total_pages": 1
  }
}
```

**Query params:**
- `page` - Page number (default: 1)
- `limit` - Items per page (default: 50, max: 100)

---

## ğŸ”„ Parallel Processing

You can create **multiple simultaneous crawls**:

```bash
# Crawl 1
curl -X POST http://localhost:3000/cep/crawl \
  -d '{"cep_start": "01001000", "cep_end": "01001009"}'

# Crawl 2 (simultaneous!)
curl -X POST http://localhost:3000/cep/crawl \
  -d '{"cep_start": "02001000", "cep_end": "02001009"}'

# Crawl 3 (simultaneous!)
curl -X POST http://localhost:3000/cep/crawl \
  -d '{"cep_start": "03001000", "cep_end": "03001009"}'
```

Each crawl:
- âœ… Has its own `crawl_id`
- âœ… Processes independently
- âœ… Maintains separate status/counters

---

## âš™ï¸ Technical Features

### Rate Limiting

- **350ms** between requests (â‰ˆ2.8 req/s)
- Uses Bottleneck for control
- Prevents ViaCEP API blocking

### Health Check

Validates ViaCEP availability before accepting crawls:

```bash
# If ViaCEP is offline:
POST /cep/crawl â†’ 503 Service Unavailable
{
  "statusCode": 503,
  "message": "CEP service is currently unavailable..."
}
```

**Automatic verification:** Every 60 seconds

### Connection Pooling

- HTTP Keep-Alive enabled (via `@nestjs/axios`)
- Reuses connections (faster)
- Max 10 simultaneous connections

---

## ğŸ—„ï¸ Data Structure

### MongoDB Collections

**crawlrequests:**
```javascript
{
  _id: ObjectId("..."),
  cep_start: "01001000",
  cep_end: "01001009",
  total_ceps: 10,
  processed_count: 10,
  success_count: 8,
  error_count: 2,
  status: "finished",
  started_at: ISODate("..."),
  finished_at: ISODate("..."),
  createdAt: ISODate("..."),
  updatedAt: ISODate("...")
}
```

**crawlresults:**
```javascript
{
  _id: ObjectId("..."),
  crawl_id: ObjectId("..."),
  cep: "01001000",
  success: true,
  data: { cep, logradouro, bairro, ... },
  error_message: null,
  retry_count: 0,
  createdAt: ISODate("...")
}
```

**Indexes:**
- `crawlrequests`: status + createdAt
- `crawlresults`: crawl_id + cep (unique)

---

## ğŸ› ï¸ Monitoring

### View Queue (ElasticMQ UI)

```
http://localhost:9325
```

### View Worker Logs

```bash
docker-compose logs -f app | grep "Processing CEP"
```

### Query MongoDB

```bash
docker-compose exec mongo mongosh

use crawler
db.crawlrequests.find().pretty()
db.crawlresults.find().limit(5).pretty()
```

---

## ğŸ› Troubleshooting

### Problem: App won't start

```bash
# View specific error
docker-compose logs app

# Common cause: missing variable in .env
# Solution: check environments/.dev.env
```

### Problem: Queue not processing

```bash
# Check if worker is running
docker-compose logs app | grep "CepWorker initialized"

# Check messages in queue
curl http://localhost:9324/000000000000/cep-queue
```

### Problem: Health check failing

```bash
# View logs
docker-compose logs app | grep "ViaCEP"

# Test manually
curl https://viacep.com.br/ws/01001000/json/
```

---

## ğŸ›ï¸ Design Patterns

### Strategy Pattern
CEP providers are interchangeable via `ICepProvider` interface.

### Template Method
`BaseHealthCheckService` defines common flow, subclasses implement `performHealthCheck()`.

### Composite Pattern
`CompositeHealthCheckService` allows multiple providers with automatic fallback.

### Dependency Injection
All components use NestJS DI for loose coupling.

---

## ğŸ“¦ Project Structure

```
src/
â”œâ”€â”€ app.module.ts              # Root module
â”œâ”€â”€ main.ts                    # Bootstrap
â”œâ”€â”€ cep/
â”‚   â”œâ”€â”€ cep.controller.ts      # REST endpoints
â”‚   â”œâ”€â”€ cep.service.ts         # Business logic
â”‚   â”œâ”€â”€ cep.worker.ts          # Queue consumer
â”‚   â”œâ”€â”€ dto/                   # Request/response DTOs
â”‚   â”œâ”€â”€ interfaces/            # Contracts (Provider, HealthCheck)
â”‚   â”œâ”€â”€ providers/             # Implementations (ViaCEP)
â”‚   â”œâ”€â”€ services/              # Health checks
â”‚   â””â”€â”€ validators/            # Custom validators
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ database.module.ts     # MongoDB config
â”‚   â””â”€â”€ mongoose-config.service.ts
â”œâ”€â”€ queue/
â”‚   â”œâ”€â”€ queue.module.ts        # SQS/ElasticMQ config
â”‚   â””â”€â”€ sqs-config.service.ts
â””â”€â”€ schemas/
    â”œâ”€â”€ crawl-request.schema.ts  # Request schema
    â””â”€â”€ crawl-result.schema.ts   # Result schema
```

---

## ğŸ” Security

- Input validation with `class-validator`
- Automatic sanitization via `ValidationPipe`
- Rate limiting to prevent abuse
- Unique indexes prevent duplicates
- Health check prevents overload

---

## ğŸ§ª Postman Collection

Import the `teste-tecnico_postman_collection.json` file into Postman for quick testing.

---

## ğŸ“ Notes

- **Range limit:** 1000 CEPs per request
- **Estimated time:** ~6 minutes for 1000 CEPs (350ms each)
- **Automatic retry:** 3 attempts via SQS DLQ
- **Invalid CEPs:** Recorded with `success: false`

---

**Documentation generated on:** 12/26/2025